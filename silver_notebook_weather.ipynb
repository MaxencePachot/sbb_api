{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3108873-e657-41ee-a1f0-27b5bb51e52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "storage_account = \"sbbapistorageaccount\"\n",
    "container = \"data-container\"\n",
    "account_key = \"\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "bronze_weather_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/weather\"\n",
    "\n",
    "# ===============================\n",
    "# 1️⃣ Load Bronze Data\n",
    "# ===============================\n",
    "df_weather_raw = spark.read.format(\"delta\").load(bronze_weather_path + \"/data\")\n",
    "df_params_raw  = spark.read.format(\"delta\").load(bronze_weather_path + \"/params\")\n",
    "\n",
    "print(\"✅ Bronze Weather Data and Parameters loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03aa85df-bec3-4db6-83f0-0542685f1eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2️⃣ Clean Parameters Table\n",
    "# ===============================\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType() \\\n",
    "    .add(\"shortname\", StringType()) \\\n",
    "    .add(\"description_de\", StringType()) \\\n",
    "    .add(\"description_fr\", StringType()) \\\n",
    "    .add(\"description_it\", StringType()) \\\n",
    "    .add(\"description_en\", StringType()) \\\n",
    "    .add(\"group_de\", StringType()) \\\n",
    "    .add(\"group_fr\", StringType()) \\\n",
    "    .add(\"group_it\", StringType()) \\\n",
    "    .add(\"group_en\", StringType()) \\\n",
    "    .add(\"granularity\", StringType()) \\\n",
    "    .add(\"decimals\", StringType()) \\\n",
    "    .add(\"datatype\", StringType()) \\\n",
    "    .add(\"unit\", StringType())\n",
    "\n",
    "# Extract the column with the raw CSV-like string\n",
    "rdd = df_params_raw.select(df_params_raw.columns[0]).rdd.map(lambda row: row[0])\n",
    "\n",
    "# Use Spark CSV parser on the RDD\n",
    "df_parsed = (\n",
    "    spark.read\n",
    "    .schema(schema)\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .csv(rdd)\n",
    ")\n",
    "\n",
    "# Select useful English columns\n",
    "df_params = df_parsed.select(\n",
    "    \"shortname\",\n",
    "    \"description_en\",\n",
    "    \"group_en\",\n",
    "    \"granularity\",\n",
    "    \"decimals\",\n",
    "    \"datatype\",\n",
    "    \"unit\"\n",
    ").na.drop()\n",
    "\n",
    "print(\"✅ Parameters table cleaned\")\n",
    "display(df_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d24a68e7-0ad0-4f2e-a0f1-71c7ee7a062b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_weather_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ab2229-3943-4392-b959-0b4fd257a84b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757684182885}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3️⃣ Rename Weather Data\n",
    "# ===============================\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Recrée un RDD depuis les lignes du fichier météo brut\n",
    "rdd_weather = df_weather_raw.select(df_weather_raw.columns[0]).rdd.map(lambda row: row[0])\n",
    "\n",
    "# Utilise Spark pour parser les lignes CSV avec `;` comme séparateur\n",
    "df_weather_parsed = spark.read.csv(\n",
    "    rdd_weather,\n",
    "    sep=\";\",\n",
    "    header=False,\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "column_names = columns_raw.split(\"_\") if len(columns_raw.split(\"_\")) == df_weather_parsed.columns.__len__() else ['_c' + str(i) for i in range(df_weather_parsed.columns.__len__())]\n",
    "\n",
    "df_weather_cleaned = df_weather_parsed.toDF(*column_names)\n",
    "\n",
    "# Colonnes fixes au début\n",
    "fixed_cols = [\"station_abbr\", \"reference_timestamp\"]\n",
    "\n",
    "# Récupérer les shortnames depuis df_params\n",
    "shortnames = [row[\"shortname\"] for row in df_params.select(\"shortname\").collect()]\n",
    "\n",
    "# Faire correspondre les colonnes du DataFrame aux shortnames\n",
    "# _c0 → station_abbr, _c1 → reference_timestamp, _c2, _c3, … → shortnames\n",
    "dynamic_cols = shortnames[:len(df_weather_cleaned.columns) - len(fixed_cols)]\n",
    "\n",
    "# Créer la liste finale des noms\n",
    "column_names = fixed_cols + dynamic_cols\n",
    "\n",
    "# Renommer les colonnes\n",
    "df_weather_cleaned = df_weather_cleaned.toDF(*column_names)\n",
    "\n",
    "# Créer le mapping shortname → description_en\n",
    "param_mapping = {\n",
    "    row[\"shortname\"]: row[\"description_en\"]\n",
    "    for row in df_params.collect()\n",
    "}\n",
    "\n",
    "# Appliquer le mapping pour remplacer les shortnames par description_en\n",
    "for i, col_name in enumerate(df_weather_cleaned.columns):\n",
    "    if col_name in param_mapping:\n",
    "        df_weather_cleaned = df_weather_cleaned.withColumnRenamed(col_name, param_mapping[col_name])\n",
    "\n",
    "print(\"✅ Weather Data columns renamed with descriptions\")\n",
    "display(df_weather_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9f09a7-e761-43d2-b519-4d60e280bb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5️⃣ Data Cleaning\n",
    "# ===============================\n",
    "# - Parse timestamp\n",
    "# - Cast numeric columns\n",
    "# - Focus on rain and snow parameters later (Silver should keep them ready)\n",
    "df_weather = (\n",
    "    df_weather\n",
    "    .withColumn(\"reference_timestamp\", F.to_timestamp(\"reference_timestamp\", \"dd.MM.yyyy HH:mm\"))\n",
    "    .withColumn(\"station_abbr\", F.col(\"station_abbr\"))\n",
    ")\n",
    "\n",
    "# Cast all numeric columns (except station_abbr and timestamp)\n",
    "for col_name in df_weather.columns:\n",
    "    if col_name not in [\"station_abbr\", \"reference_timestamp\"]:\n",
    "        df_weather = df_weather.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "\n",
    "print(\"✅ Weather Data cleaned and typed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd3dbf4-d4b7-4b2d-81e4-9b16d35b66c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6️⃣ Clean column names\n",
    "# ===============================\n",
    "import re\n",
    "\n",
    "# Fonction pour nettoyer un nom de colonne\n",
    "def clean_column_name(name):\n",
    "    # Remplace les caractères invalides par '_'\n",
    "    name = re.sub(r\"[ ,;()/]\", \"_\", name)\n",
    "    # Supprime les '_' doublons\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    # Supprime '_' en début et fin\n",
    "    name = name.strip(\"_\")\n",
    "    return name\n",
    "\n",
    "# Appliquer sur toutes les colonnes\n",
    "for col_name in df_weather_cleaned.columns:\n",
    "    df_weather_cleaned = df_weather_cleaned.withColumnRenamed(col_name, clean_column_name(col_name))\n",
    "\n",
    "# Vérification\n",
    "print(\"✅ Colonnes nettoyées pour Delta :\")\n",
    "display(df_weather_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b30b038-c871-4bde-bd38-67657ec65943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7 Save Silver data\n",
    "# ===============================\n",
    "silver_weather_path_data = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/weather/data\"\n",
    "\n",
    "(df_weather_cleaned.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .save(silver_weather_path_data))\n",
    "\n",
    "print(\"✅ Silver Weather created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d6c8d99-dc60-4858-93e2-94f37d74578d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7 Save Silver params\n",
    "# ===============================\n",
    "silver_weather_path_params = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/weather/params\"\n",
    "\n",
    "(df_params.write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .save(silver_weather_path_params))\n",
    "\n",
    "print(\"✅ Silver Weather created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72789f2a-5e43-40e1-b6c4-c769cff467bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8 Register SQL tables\n",
    "# -------------------------------\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver_weather_data\n",
    "USING DELTA\n",
    "LOCATION '{silver_weather_path_data}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver_weather_parameters\n",
    "USING DELTA\n",
    "LOCATION '{silver_weather_path_params}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3714568-01ca-4335-8e30-1bfaf1975016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM silver_weather_parameters\n",
    "LIMIT 3\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_notebook_weather",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
