{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5d0c09-affa-4456-933b-086c1db368ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Bronze SBB Ingestion\n",
    "# Objective: read JSON from ADLS Gen2 and create Bronze in Delta Lake\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Import SparkSession\n",
    "# -------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f004807-2824-49dc-8333-3ccaa020506c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2️⃣ Define ADLS variables\n",
    "# -------------------------------\n",
    "storage_account = \"sbbapistorageaccount\"\n",
    "container = \"data-container\"\n",
    "account_key = \"\"\n",
    "\n",
    "# Configure Spark which allows it to access the ADLS Gen2 storage account\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "# Path of JSON file\n",
    "adls_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/data/sbb.json\"\n",
    "print(\"Chemin JSON :\", adls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acd4155-9aa7-4ccf-97ad-d1f62eae9e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Read JSON\n",
    "# -------------------------------\n",
    "# Using badRecordsPath option to catch the errors\n",
    "df_raw = spark.read.option(\"multiLine\", True).json(adls_path)\n",
    "\n",
    "# Verify 5 first rows and schema\n",
    "df_raw.show(5, truncate=False)\n",
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d466c0d5-b5a8-4a08-bb9c-4139dd8d78c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "df_clean = df_raw.select(\n",
    "    explode(col(\"results\")).alias(\"result\"),  # converts each element of the array into a line\n",
    "    col(\"total_count\")\n",
    ")\n",
    "\n",
    "# Now that result is a struct, we can extract its fields\n",
    "df_clean = df_clean.select(\n",
    "    col(\"result.*\"), \n",
    "    col(\"total_count\")\n",
    ")\n",
    "\n",
    "df_clean.show(1, truncate=False)\n",
    "#df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6428aeb4-4032-45b5-a87b-e566ae2b0d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5️⃣ Define Bronze path\n",
    "# -------------------------------\n",
    "bronze_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/bronze/sbb\"\n",
    "print(\"Chemin Bronze :\", bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef59f16-36eb-434a-bbc4-b60805f60865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6️⃣ Write Bronze in Delta Lake\n",
    "# -------------------------------\n",
    "df_clean.write.format(\"delta\").option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(bronze_path)\n",
    "\n",
    "print(\"✅ Bronze SBB créée avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f9104b-b8f3-4809-ab94-a856774be2bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 7️⃣ Verification\n",
    "# -------------------------------\n",
    "df_bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "df_bronze.show(3, truncate=False)\n",
    "#df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beba79f2-b474-4729-b3b3-4d7afda0051b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 8️⃣ Optionnal : Create Bronze SQL table\n",
    "# -------------------------------\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bronze_sbb\n",
    "USING DELTA\n",
    "LOCATION '{bronze_path}'\n",
    "\"\"\")\n",
    "print(\"✅ Table SQL bronze_sbb créée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6152379c-0094-4726-a443-81e88257c0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM bronze_sbb\n",
    "LIMIT 3\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_notebook_sbb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
